{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCu72vDHGMHo"
   },
   "source": [
    "## **| 모델 훈련 연습 문제**\n",
    "___\n",
    "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
    "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3g-_Dq9GiuT"
   },
   "source": [
    "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
    "\n",
    "확률적 경사하강법, 미니배치 경사 하강법을 사용할 수 있다.\n",
    "정규방정식의 경우 계산 복잡도가 n에 따라 가파르게 증가하기 때문에 사용하기에 적절하지 않다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pDjW5XcHPOt"
   },
   "source": [
    "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
    "학습률이 너무 높아 알고리즘이 발산하고 있을 가능성이 높다. 오버피팅의 문제이므로 검증 에러가 최솟값에 도달했을 때 학습을 조기종료하며 해결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM7JbsLoy7b7"
   },
   "source": [
    "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
    "훈련 오차와 검증 오차가 비슷하고 둘다 높다면 모델이 훈련 세트에 과소적합 되었을 가능성이 있다. 높은 편향을 가진 모델이므로 규제 하이퍼 파라미터를 감소시켜야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8tARu-ZzOGx"
   },
   "source": [
    "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
    "___\n",
    "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
    "\n",
    "규제가 있는 모델이 규제가 없는 모델에 비해 성능이 좋기 때문이다.\n",
    "- 릿지 회귀 대신 라쏘 회귀\n",
    "\n",
    "라쏘 회귀는 패널티를 통해 작은 가중치를 0으로 만드는 경향이 있다. 가장 중요한 가중치를 제외하고는 모두 0으로 만들기 때문에 희소한 모델을 만든다. 이는 자동 특성 선택의 효과를 가지므로 몇개의 특정한 값만 실제 유용할 것이라고 판단될 때 사용하기 적합하다.\n",
    "- 라쏘 회귀 대신 엘라스틱넷\n",
    "\n",
    "특성이 몇개 강하게 연관되어 있거나 n이 훈련 샘플보다 많을 때 라쏘가 불규칙적으로 행동될 수 있기 때문에 엘라스틱넷을 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIZpOEYJVIAV"
   },
   "source": [
    "### **추가) 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYCOM\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "X=iris['data'][:,(2,3)]\n",
    "y=iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X와 1 shape의 array 와 cbind 하기\n",
    "import numpy as np\n",
    "X_b=np.c_[np.ones([len(X),1]),X]\n",
    "np.random.seed(1)\n",
    "# 랜덤 시드 고정\n",
    "# train_test_split 만들기 \n",
    "test_ratio=0.2\n",
    "validation_ratio=0.2\n",
    "total_size=len(X_b)\n",
    "test_size=int(total_size*test_ratio)\n",
    "validation_size=int(total_size*validation_ratio)\n",
    "train_size=total_size-test_size-validation_size\n",
    "# 무작위 순서 바꾸기\n",
    "rn_idx=np.random.permutation(total_size)\n",
    "X_train=X_b[rn_idx[:train_size]]\n",
    "y_train=y[rn_idx[:train_size]]\n",
    "X_valid=X_b[rn_idx[train_size:-test_size]]\n",
    "y_valid=y[rn_idx[train_size:-test_size]]\n",
    "X_test=X_b[rn_idx[-test_size:]]\n",
    "y_test=y[rn_idx[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 2,\n",
       "       1, 2, 1, 2, 2, 1, 2, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 회귀 모델을 훈련시키기 위해선 타깃 클래스의 확률을 알아야 함\n",
    "# 확률이 1인 타깃 클래스를 제외한 다른 클래스의 확률은 0\n",
    "# one_hot 벡터로 바꿔주기\n",
    "def one_hot(y):\n",
    "    n_classes=y.max()+1\n",
    "    m=len(y)\n",
    "    Y_onehot=np.zeros((m,n_classes))\n",
    "    Y_onehot[np.arange(m),y]=1\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y 값 모두 바꿔주기\n",
    "\n",
    "y_train_onehot=one_hot(y_train)\n",
    "y_test_onehot=one_hot(y_test)\n",
    "y_valid_onehot=one_hot(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 함수 쓰기\n",
    "def softmax(x):\n",
    "    exp=np.exp(x)\n",
    "    exp_s=np.sum(exp,axis=1,keepdims=True)\n",
    "    return exp/exp_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.339528736087666\n",
      "500 0.9365396657020225\n",
      "1000 0.7516536811842142\n",
      "1500 0.6608878343187011\n",
      "2000 0.6119789827398047\n",
      "2500 0.5829349541862657\n",
      "3000 0.5640900120092367\n",
      "3500 0.5509438596703224\n",
      "4000 0.5412512816873222\n",
      "4500 0.5338052539685751\n"
     ]
    }
   ],
   "source": [
    "# 크로스 엔트로피 비용함수\n",
    "eta=0.01\n",
    "n_iterations=5000\n",
    "m=len(X_train)\n",
    "alpha=0.1\n",
    "best_loss=np.infty #조기 종류 추가 \n",
    "Theta=np.random.randn(3,3)\n",
    "for iteration in range(n_iterations):\n",
    "    logits=X_train.dot(Theta)\n",
    "    y_proba=softmax(logits)\n",
    "    entropy_loss=-np.mean(np.sum(y_train_onehot*np.log(y_proba+1e-7),axis=1))\n",
    "    l2_loss=1/2*np.sum(np.square(Theta[1:]))\n",
    "    loss=entropy_loss+alpha*l2_loss\n",
    "    error=y_proba-y_train_onehot\n",
    "    gradients=1/m*X_train.T.dot(error)+np.r_[np.zeros([1,3]),alpha*Theta[1:]]\n",
    "    Theta=Theta-eta*gradients\n",
    "    logits=X_valid.dot(Theta)\n",
    "    y_proba=softmax(logits)\n",
    "    entropy_loss=-np.mean(np.sum(y_valid_onehot*np.log(y_proba+1e-7),axis=1))\n",
    "    l2_loss=1/2*np.sum(np.square(Theta[1:]))\n",
    "    loss=entropy_loss+alpha*l2_loss\n",
    "    if iteration%500==0:\n",
    "        print(iteration,loss)\n",
    "    if loss<best_loss:\n",
    "        best_loss=loss\n",
    "    else:\n",
    "        print(iteration-1,best_loss)\n",
    "        print(iteration,loss,'Early Stopping-!')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
